---
layout: post
title:  "Java爬虫入个门"
date:   2019-05-09 22:10:00 +0800
categories: Java,Kotlin,WebMagic,sql
---

## Java爬虫
### 我理解的爬虫结构
#### 网页请求

如果要爬取一个网站，首先就得向她发送请求，让她把你需要的数据反馈回来。这就需要一个模块，来模拟请求，然后下载网站返回的网页数据。Java中，最简单基础的此类工具便是[Apache HttpClient](http://hc.apache.org/index.html) 了。当然还有一些封装的其余轮子。最终目的就是让网站服务器将我们要的网页发送给我们。由于只是初步了解爬虫，此环节，我想到的难点有两个吧：
- 网页需要登陆、或者验证码之类的校验过程。
- 网页是使用前端渲染方式显示的，一个请求过去，返回一堆JS代码。
解决好以上两个问题，大部分普通网页都能搞到手了。

#### 解析网页

上一个模块中，拿到了网页的html数据。接下来，就是在这大片的html中，挑出自己需要的数据信息。Java中，当然第一时间想到了[Jsoup](http://jsoup.org/) 了，用它再加上正则表达式、Xpath等工具，通过对目标网页的特征分析，不难解析出自己需要的信息。但是也需要注意一些丧心病狂的html混淆技术。

#### 连锁控制

都叫爬虫了，当然得爬得起来才行。不能就抓那几个页面，解析解析就行了。一般为了获取某个网站的数据信息，得根据网页中出现的链接不断循环跳转，如同蜘蛛网一般，从某一条丝线开始，爬遍整张网。所以，需要一个控制模块，管理需要连锁抓取的链接队列、去掉爬过的链接（防止死循环）。Java中，最简单的实现就是用队列，在内存中保持一个将要爬取的URL队列。然后使用集合等来直接去重。

#### 结果处理

上一步，通过网页中出现的URL线索，逐步爬遍了整个网站，并且在爬取过程中，解析出了我们需要的数据信息。接下来，就需要一个模块来处理我们的这些可能大量的信息。比如计算、持久化、输出到屏幕、可视化处理等。一般直接保存到数据库中，然后再进一步使用这些数据。

#### 总体控制

上面四部分互相协作，就构成了一个简单的爬虫程序，但是还是需要使用一定的设计模式，将他们有机结合在一起，方便扩展。而且抓取数据过程的线程使用控制、启动暂停、爬取过程监控等功能也需要一个模块来协调。至此，一个基本完整的简单爬虫就成型了，至于一些分布式等处理，就暂且不议了。（其实不会）

### 造好的轮子
